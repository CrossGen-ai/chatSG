# ChatSG Backend Configuration

# ===========================================
# BACKEND ROUTING CONFIGURATION
# ===========================================
# Choose your backend mode: Orch (recommended), n8n, or Generic
BACKEND=Orch
ENVIRONMENT=dev

# ===========================================
# LLM PROVIDER CONFIGURATION
# ===========================================
# The LLM helper will automatically detect which provider to use
# based on which credentials are provided

# --- Option 1: Regular OpenAI ---
# Uncomment these lines to use regular OpenAI
# OPENAI_API_KEY=your_openai_api_key_here
# OPENAI_MODEL=gpt-4o-mini
# OPENAI_BASE_URL=https://api.openai.com/v1

# --- Option 2: Azure OpenAI ---
# Uncomment these lines to use Azure OpenAI
# AZURE_OPENAI_API_KEY=your_azure_api_key_here
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
# AZURE_OPENAI_DEPLOYMENT=gpt-4o-001
# AZURE_OPENAI_API_VERSION=2024-02-15-preview

# ===========================================
# LLM BEHAVIOR CONFIGURATION
# ===========================================
# Optional: Override default LLM settings
# LLM_TEMPERATURE=0.7
# LLM_MAX_TOKENS=3000

# ===========================================
# N8N WEBHOOK CONFIGURATION
# ===========================================
# Required when BACKEND=n8n
# WEBHOOK_URL=http://localhost:5678/webhook/chat

# ===========================================
# DEVELOPMENT NOTES
# ===========================================
# 
# Backend Modes:
# - Orch: Intelligent orchestration system (RECOMMENDED - default)
#   * Provides intelligent agent selection and routing
#   * Handles n8n and Generic backends through orchestration
#   * Best performance and scalability
# - n8n: Direct webhook forwarding (requires WEBHOOK_URL)
# - Generic: Simulated responses for development (no LLM required)
#
# LLM Provider Priority:
# 1. If AZURE_OPENAI_API_KEY + AZURE_OPENAI_ENDPOINT exist → Azure OpenAI
# 2. If OPENAI_API_KEY exists → Regular OpenAI
# 3. Default to OpenAI (will fail without credentials)
#
# Environment-based defaults:
# - production: Lower temperature (0.3), fewer tokens (2000)
# - development: Higher temperature (0.7), more tokens (4000)
# - default: Balanced settings (0.5 temp, 3000 tokens) 